{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6139a1b-0eb5-401e-a1b0-1aa9fc044a2e",
   "metadata": {},
   "source": [
    "# Part 2: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad80ee-2888-4555-830b-cc3879e1693a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook Summary\n",
    "\n",
    "This notebook shows the reader how an API webscraper was developed to collect text posts from two different subreddits. Although the code is written and commented here to show the overall development of the webscraper and the API request, the executable Python code is saved in a separate .py file for the user to implement in their own Terminal. Included in this notebook, the reader will find:\n",
    "\n",
    "* Library Imports\n",
    "* User Credentials and Authorization Functions\n",
    "* Webscraper and Transaction Log Functions\n",
    "* Main Function\n",
    "* Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c77259-eca5-440c-aa01-4a27fcff9eb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Library Imports\n",
    "\n",
    "First, we will import the requisite libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34e9208-2a8f-454d-bf30-2729588b783b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35354e4-5926-4038-b1e4-13901d5e4686",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## User Credentials and Authorization Functions\n",
    "\n",
    "Next, we will gather user input regarding their authorization credentials for accessing the Reddit API as well as which two subreddits the user wishes to scrape for data. The first function will ask the user for their credentials, and store them for subsequent access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6861fe06-5d6c-43bb-b9aa-96f4b144e9b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client_id = 'w0zou506jc-kPrtfSvQBSA'\n",
    "# client_secret =  'Yv54-psOKLENM4nRWHNOmiU1w5y2aQ'\n",
    "# user_agent =  'project_3'\n",
    "# username =  'the_nerdist_1'\n",
    "# password =  'rhd@afw7WRZ@cmv2rzq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5406ab4-d0c6-4336-8969-b0a5f7dee05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gather_user_input():\n",
    "    # asks the user to provide two subreddits of interest and authorization credentials\n",
    "    subred_1 = input('What is the first subreddit from which you would like to collect data? ')\n",
    "    subred_2 = input('What is the second subreddit from which you would like to collect? ')\n",
    "    client_id = input('What is your API client ID? ')\n",
    "    client_secret =  input('What is your API client secret? ')\n",
    "    user_agent =  input('What is the user agent? ')\n",
    "    username =  input('What is the user name? ')\n",
    "    password =  input('What is the password? ')\n",
    "    \n",
    "    # stores all user data solicited above into a dictionary\n",
    "    user_info = [{\n",
    "    'subred_1': subred_1,\n",
    "    'subred_2': subred_2,\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'user_agent': user_agent,\n",
    "    'username': username,\n",
    "    'password': password\n",
    "    }]\n",
    "    \n",
    "    # stores the user data dictionary into a a json file to be read later\n",
    "    pd.DataFrame(user_info).to_json('./data_files/user_info.json')\n",
    "    \n",
    "    # prints a message to the user indicating credentials stored or throws error if failed\n",
    "    try:\n",
    "        if os.path.exists('./data_files/user_info.json'):\n",
    "            print(\"User credentials successfully stored.\")\n",
    "    except Exception as E:\n",
    "        print(f'Error is {E}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bebad8-67b5-4772-8e44-f2dfb4008eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def authorize_creds():\n",
    "    # reads in json file with stored user credential data\n",
    "    user_info = pd.read_json('./data_files/user_info.json')\n",
    "    \n",
    "    # uses user credentials to define authorization and data access\n",
    "    auth = requests.auth.HTTPBasicAuth(user_info.loc[0, 'client_id'], user_info.loc[0, 'client_secret'])\n",
    "    data = {\n",
    "        'grant_type': 'password',\n",
    "        'username': user_info.loc[0, 'username'],\n",
    "        'password': user_info.loc[0, 'password']\n",
    "    }\n",
    "    #create an informative header for your application\n",
    "    headers = {'User-Agent': 'joey/0.0.1'}\n",
    "    \n",
    "    # submits request to API\n",
    "    res = requests.post(\n",
    "        'https://www.reddit.com/api/v1/access_token',\n",
    "        auth=auth,\n",
    "        data=data,\n",
    "        headers=headers)\n",
    "    \n",
    "    # prints response code to user, 200 if status ok\n",
    "    print(res)\n",
    "    \n",
    "    #retrieve access token\n",
    "    token = res.json()['access_token']\n",
    "\n",
    "    # creates access token in headers for scraping data\n",
    "    headers['Authorization'] = f'bearer {token}'\n",
    "    \n",
    "    # submits request to API\n",
    "    requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).status_code == 200\n",
    "    \n",
    "    # returns new header with access token for use in webscraper\n",
    "    return headers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f95bce7-a76b-4ff8-8efe-18533c91e696",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Webscraper and Transaction Log Functions\n",
    "\n",
    "Now that we had set up functions to solicit user input on the subreddits of interest and authorized user credentials, we can use the requests to the Reddit API to begin collecting data. While we collect our data on a daily basis, we will also build a separate transation log to track the date and time of each API get request, the amount of posts collected in each individual script run, and the total posts collected to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8f1d21-dc83-4da1-a748-d7f6f84df4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def api_scrape(headers):\n",
    "    # reads in the user credentials to pull the two subreddits of interest\n",
    "    user_info = pd.read_json('./data_files/user_info.json')\n",
    "    base_url = 'https://oauth.reddit.com/r/'\n",
    "    subreddits = [user_info.loc[0, 'subred_1'], user_info.loc[0, 'subred_2']]\n",
    "    \n",
    "    # create an empty list to append a text dictionary of one post at a time\n",
    "    posts = []\n",
    "    \n",
    "    # creates a for loop to iterate through both of the subreddits of interest\n",
    "    for subreddit in subreddits:\n",
    "        \n",
    "        # creates a for loop to collect up to 100 posts at a time 10 separate times\n",
    "        for i in range(10):\n",
    "            # sets post collection limit to 100 per API restrictions\n",
    "            if i == 0:\n",
    "                params = {\n",
    "                    'limit': 100\n",
    "                }\n",
    "            # pagination set after first iteration to gather next 100 posts\n",
    "            else:\n",
    "                params = {\n",
    "                    'after': after_param,\n",
    "                    'limit': 100\n",
    "                }\n",
    "            \n",
    "            # submits request to API and stores in res variable\n",
    "            res = requests.get(base_url+subreddit, \n",
    "                               headers=headers, \n",
    "                               params = params)\n",
    "            \n",
    "            # loops through all the subreddit posts in the get request, up to 100\n",
    "            for j in range(len(res.json()['data']['children'])):\n",
    "                # creates an empty dictionary each post and stores title, selftext, and subreddit\n",
    "                text = {}\n",
    "                text['title'] = res.json()['data']['children'][j]['data']['title']\n",
    "                text['selftext'] = res.json()['data']['children'][j]['data']['selftext']\n",
    "                text['subreddit'] = res.json()['data']['children'][j]['data']['subreddit']\n",
    "                # appends the whole text dictionary to the post list if it is not a duplicate title\n",
    "                if text['title'] not in posts:\n",
    "                    posts.append(text)\n",
    "            \n",
    "            # sets the after parameter for next 100 posts if it exists; otherwise ends for loop\n",
    "            try:\n",
    "                after_param = res.json()['data']['children'][-1]['data']['name']\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # checks if csv file storing all posts already exists\n",
    "    # if csv file does exist, reads in the file, creates a df of the posts just created,\n",
    "    # concatenates the two dfs, stores as csv file, and then accesses transaction log\n",
    "    if os.path.exists('./data_files/subreddit_posts.csv'):\n",
    "        subreddit_posts = pd.read_csv('./data_files/subreddit_posts.csv')\n",
    "        new_posts = pd.DataFrame(posts)\n",
    "        subreddit_posts = pd.concat([subreddit_posts, new_posts], ignore_index = True)\n",
    "        subreddit_posts.to_csv('./data_files/subreddit_posts.csv', index = False)\n",
    "        transaction_log(posts, subreddit_posts)\n",
    "    # if csv file does not exist, creates a df of the posts just created, stores as csv file,\n",
    "    # then accesses transaction log\n",
    "    else:\n",
    "        subreddit_posts = pd.DataFrame(posts)\n",
    "        subreddit_posts.to_csv('./data_files/subreddit_posts.csv', index = False)\n",
    "        transaction_log(posts, subreddit_posts)\n",
    "    \n",
    "    # returns the df for user to see\n",
    "    return subreddit_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3c519-722b-4f8f-a903-ba528b51d2c8",
   "metadata": {},
   "source": [
    "Within the api_scrape function, we call the transaction log function. The purpose of the transaction log is to create a separate csv file in which the user can see when the webscraper was run to collect subreddit posts, how many posts were collected when the script was run, and how many total posts have been collected to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fc2a44-d2cf-4486-9a4b-c278d854864a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transaction_log(posts, subreddit_posts):\n",
    "    # creates an empty list to store new log entry as a dictionary\n",
    "    new_log = []\n",
    "    \n",
    "    # if transaction log already exists, reads in the log,\n",
    "    # creates a new log entry as a list with a dictionary, \n",
    "    # saves new log entry as a df, concatenates both logs,\n",
    "    # then saves transaction log as csv\n",
    "    if os.path.exists('./data_files/transaction_log.csv'):\n",
    "        trans_log = pd.read_csv('./data_files/transaction_log.csv')\n",
    "        current_log = [{'datetime_retrieved': datetime.datetime.now(), \n",
    "                       'posts_retrieved': len(posts), \n",
    "                       'total_posts': subreddit_posts.shape[0]}]\n",
    "        new_log = pd.DataFrame(current_log)\n",
    "        trans_log = pd.concat([trans_log, new_log], ignore_index = True)\n",
    "        trans_log.to_csv('./data_files/transaction_log.csv', index = False)\n",
    "    # If not transaction log exists yet, \n",
    "    # creates a new log entry as a list with a dictionary,\n",
    "    #saves new log entry as a df, then saves transaction log as csv\n",
    "    else:\n",
    "        current_log = [{'datetime_retrieved': datetime.datetime.now(), \n",
    "                       'posts_retrieved': len(posts), \n",
    "                       'total_posts': subreddit_posts.shape[0]}]\n",
    "        trans_log = pd.DataFrame(current_log)\n",
    "        trans_log.to_csv('./data_files/transaction_log.csv', index = False)\n",
    "    \n",
    "    # prints out transaction log status to user\n",
    "    return print(f'There were {len(posts)} retrieved at {datetime.datetime.now()} for a total of {subreddit_posts.shape[0]} to date.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba724f9c-4d56-4fb7-9f2f-3406b6d9606c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Main Function\n",
    "\n",
    "The main function will exexcute after all other prior functions have been defined. The main function check is the user_info json file exists and determine whether to gather user input first or simply execute credential authorization and the API webscrape script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed61110d-7462-4245-a7c3-76581bf665d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "There were 1759 retrieved at 2023-08-09 14:13:04.464410 for a total of 18115 to date.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18115, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the user info json file exists, authorize credentials\n",
    "if os.path.exists('./data_files/user_info.json'):\n",
    "    headers = authorize_creds()\n",
    "# if the user info json file does not exists, gather user input,\n",
    "# then authorize credentials\n",
    "else:\n",
    "    gather_user_input()\n",
    "    headers = authorize_creds()\n",
    "\n",
    "# call webscrape function to run script and return response status and log status\n",
    "subreddit_posts = api_scrape(headers)\n",
    "\n",
    "# print new df shape\n",
    "subreddit_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b05988e-4ac8-4731-bf42-46cf9a6730b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Megathread: US Medication Shortage</td>\n",
       "      <td>As many of you are aware by now, the current U...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did you do something you're proud of? Somethin...</td>\n",
       "      <td>What success have you had this week?\\n\\nDid yo...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Vyvanse poops have taken over my mornings..</td>\n",
       "      <td>I now wake up at least 1.5 hours early to ensu...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why does someone forcing you to push through e...</td>\n",
       "      <td>I can’t even explain how it hurts but it’s so ...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just had an epiphany- isn’t it crazy how relig...</td>\n",
       "      <td>So my mom can believe in all her saints, God, ...</td>\n",
       "      <td>ADHD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18110</th>\n",
       "      <td>Me when I friend zone someone by accident</td>\n",
       "      <td>I friend zoned a girl for 5 years once by acci...</td>\n",
       "      <td>autism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18111</th>\n",
       "      <td>Trying to come off as less rude at work</td>\n",
       "      <td>Hello, this is my first post here. For clarifi...</td>\n",
       "      <td>autism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18112</th>\n",
       "      <td>Do you feel hyper sensitive to negative affect?</td>\n",
       "      <td>I don't always place specifics, but I seem to ...</td>\n",
       "      <td>autism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18113</th>\n",
       "      <td>Limiting food intake</td>\n",
       "      <td>Hello, do any of you purposely not eat all you...</td>\n",
       "      <td>autism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18114</th>\n",
       "      <td>Does going to school ever get easier?</td>\n",
       "      <td>Last year I suddenly started struggling with g...</td>\n",
       "      <td>autism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18115 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0                     Megathread: US Medication Shortage   \n",
       "1      Did you do something you're proud of? Somethin...   \n",
       "2        The Vyvanse poops have taken over my mornings..   \n",
       "3      Why does someone forcing you to push through e...   \n",
       "4      Just had an epiphany- isn’t it crazy how relig...   \n",
       "...                                                  ...   \n",
       "18110          Me when I friend zone someone by accident   \n",
       "18111            Trying to come off as less rude at work   \n",
       "18112    Do you feel hyper sensitive to negative affect?   \n",
       "18113                               Limiting food intake   \n",
       "18114              Does going to school ever get easier?   \n",
       "\n",
       "                                                selftext subreddit  \n",
       "0      As many of you are aware by now, the current U...      ADHD  \n",
       "1      What success have you had this week?\\n\\nDid yo...      ADHD  \n",
       "2      I now wake up at least 1.5 hours early to ensu...      ADHD  \n",
       "3      I can’t even explain how it hurts but it’s so ...      ADHD  \n",
       "4      So my mom can believe in all her saints, God, ...      ADHD  \n",
       "...                                                  ...       ...  \n",
       "18110  I friend zoned a girl for 5 years once by acci...    autism  \n",
       "18111  Hello, this is my first post here. For clarifi...    autism  \n",
       "18112  I don't always place specifics, but I seem to ...    autism  \n",
       "18113  Hello, do any of you purposely not eat all you...    autism  \n",
       "18114  Last year I suddenly started struggling with g...    autism  \n",
       "\n",
       "[18115 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print new df\n",
    "subreddit_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fb2c6-1366-4efe-b73c-d6f458e6f9f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook Conclusion\n",
    "\n",
    "In this notebook, we built a series of functions to gather user input, authorize user credentials, webscrape the Reddit API, and gather up to 1000 unique user posts from each of two different subreddits at a time. In the process, this script creates a transaction log to keep track of our prior script executions. This script is intended to be run in the Terminal, so again, a separate Python file has been created in this report while this notebook is preserved to provide more detail and explanation to the reader of how data were collected.\n",
    "\n",
    "In Part 3, we will begin the process of data cleaning and exploratory data analysis based on the posts that we have collected from this script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
